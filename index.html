<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Hypothesis Tests</title>
    <meta charset="utf-8" />
    <meta name="author" content="Adi Sarid" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/rutgers-fonts.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Hypothesis Tests
## Lecture #4
### Adi Sarid
### Tel-Aviv University
### updated: 2019-11-17

---


&lt;style type="text/css"&gt;
.remark-code, .remark-inline-code {
  background: #f0f0f0;
}
.remark-code {
  font-size: 24px;
}

.huge { 
  font-size: 200%;
}
.tiny .remark-code {
  font-size: 50%;
}

.small .remark-code{
   font-size: 85% !important;
}

.small {
   font-size: 85% !important;
}

.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}

table { display: inline-block; }

th, td {
   padding: 5px;
}

.small-slide {
   font-size: 70% !important;
}

.image-50 img {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}

.right-plot {
   width: 60%;
   float: right;
   padding-left: 1%;
   bottom: 0px;
   right: 0px;
   position: absolute;
}



&lt;/style&gt;



# Reminder from previous lecture

Last lesson we talked about:

--

   * Confidence intervals, i.e.:
   
`$$P(\bar{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{n}} &lt; \mu &lt; \bar{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}}) = 1-\alpha$$`

--

   * We've seen confidence intervals for
   
      * Normal distribution (for `\(\mu\)` and for `\(\sigma\)`)
      
      * Using Student's T (variance unknown)
      
      * Binomial case (the election survey example)
      
`$$\hat{p}\pm \frac{z_{\alpha/2}}{2\sqrt{n}}$$`

--

   * Setting the sample size according to a confidence interval length, e.g.:
   
`$$n\geq \left(z_{\alpha/2}\frac{\sigma}{r}\right)^2$$`

   * Prediction intervals
   
      * Used when we want to express the uncertainty of the *next observation*

`$$\bar{X}-t_{\alpha/2,n-1}\times s\sqrt{1+\frac{1}{n}}\leq x_{n+1} \leq \bar{X} + t_{\alpha/2,n-1}\times s\sqrt{1+\frac{1}{n}}$$`

---

# Hypothesis testing (Montgomery chapter 9)

A *statistical hypothesis* is a statement about the parameters of one or more populations.

--

In empirical research, we first formulate our hypothesis, and then we try to find empirical results to support our hypothesis (never the other way around, that's called HARK-ing).

--

For example:

  * `\(H_0\)`: The average time to reach TLV from Netanya `\(=\)` 40 minutes
  
  * `\(H_1\)`: The average time to reach TLV from Netanya `\(\neq\)` 40 minutes 
  
--
  
The `\(H_0\)` is called the *null hypothesis* and the `\(H_1\)` is called the *alternative hypothesis*.

--

The same situation can be descrived with different hypothesis (with a different meaning):

  * `\(H_0\)`: The average time to reach TLV from Netanya `\(=\)` 40 minutes
  
  * `\(H_1\)`: The average time to reach TLV from Netanya `\(&gt;\)` 40 minutes

--

Today we will discuss how to devise hypothesis tests, what are type-I and type-II errors, what is the meaning of rejecting a null hypothesis, what are p-values and what is the connection to the statistical intervals we were discussing.

---

# First - an example

Scan the following QR code (or visit the link) and answer the survey.

[http://bit.ly/att_flu_ex](http://bit.ly/att_flu_ex)

![](images/link_for_survey_example.png)

<div class="countdown" id="timer_5dd0781a" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">05</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

--

This is a copy of a true survey used in a research I did with a collegue.

   * What do you think were our hypothesis in this survey?
   
   * What were we trying to accomplish?

---

# The Attractive Flu Shot (1/2)

The survey has a number of versions, rendered to respondents randomly. There are five groups:

   * Control (a "regular" message from the HMO)
   
   * Recommendation (for effectiveness of the shot, the health ministry recommends to take it early)
   
   * Stock (if you don't take an early shot, the stock may run out)
   
   * Cost (the shot would cost for patients taking it after December)
   
   * Benefit (if you take an early shot, you get some kind of incentive)

--

This psychological nudge leverages the *attraction effect*, i.e.: options *A* and *B* are not comparable, but when decoy *b* is added, and is comparable to *B*, we tend to choose *B* over *A*.

&lt;img src="03-Hypothesis_tests_files/figure-html/nudge example-1.png" style="display: block; margin: auto;" /&gt;

---

# The Attractive Flu Shot (2/2)

If you read Dan Arieli's books, you probably read about attraction.

We have a lot of hypothesis in this research, but here is an example (the treatment increases vaccination intentions):

   * Recommendation treatment:
       
      * `\(H_0\)`: `\(p_{\text{control}}=p_{\text{recommendation}}\)`
      * `\(H_1\)`: `\(p_{\text{control}}&lt;p_{\text{recommendation}}\)`

--

   * Stock treatment:
   
      * `\(H_0\)`: `\(p_{\text{control}}=p_{\text{stock}}\)`
      * `\(H_1\)`: `\(p_{\text{control}}&lt;p_{\text{stock}}\)`

--
      
   * You get the hang of it...
   
--

Additional hypothesis deal with the interaction of *certainty* and the attraction effect's influence.

---

# Back to theory of hypothesis testing

Let's simplify things: say that the percent of patients taking flu vaccinations is about 20% (known based on previous years). We want to see if our experiment led to an increase in that percent, that is *significantly* higher.

   * `\(H_0\)`: `\(p_\text{treatment} = 0.2\)`
   
   * `\(H_1\)`: `\(p_\text{treatment} &gt; 0.2\)`
   
--

What would you say if we measure after the intervention the following rates...?

   * `\(\hat{p} = 0.19\)`

--

   * `\(\hat{p} = 0.60\)`

--
   
   * `\(\hat{p} = 0.25\)`
   
--

We need a clear statistical *criteria* for deciding what is significant and what is not.

--

class: small-slide

&lt;img src="03-Hypothesis_tests_files/figure-html/binomial distribution and criteria-1.png" style="display: block; margin: auto;" /&gt;

---

# Two types of errors

In order to set a decision rule, we need to consider two types of errors:

   * **Type-I** error (aka False-positive): **rejecting** `\(H_0\)` when it is **true**.
   
   * **Type-II** error (aka False-negative): **failing** to reject `\(H_0\)` when it is **false**.
   
--

In medical decision making, this would look like `\(H_0\)`: not pregnant
.image-50[
![](images/Type_IandType_II_errors.jpg)
]
[(source)](http://www.statisticssolutions.com/to-err-is-human-what-are-type-i-and-ii-errors/])

---

# Tradeoff between type-I and type-II errors

### Type-I error

The type-I error is also called the *significance level*, or `\(\alpha\)`-error.

`$$\alpha = P(\text{Reject } H_0| H_0 \text{ True})$$`

<div class="countdown" id="timer_5dd076d2" style="top:0;right:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">01</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

Questions:

   * In the pregnancy classification example, what is an `\(\alpha=0\)` decision rule?
   
   * In the flu vaccination example, what is an `\(\alpha=0\)` decision rule?
   
--

Answers:

   * Always classify: not pregnant
   
   * Reject `\(H_0\)` when `\(\hat{p}=\pm\infty\)` (i.e. never reject `\(H_0\)`)

--

### Type-II error

As the type-I error decreases the decision rule tends to prefer not rejecting `\(H_0\)` which leads to a higher type-II error

`$$\beta=P(\text{Fail to reject } H_0| H_1 \text{ True})$$`

---

# General framework for hypothesis testing

This is the procedure for hypothesis testing:

   1. Identify the parameter of interest (i.e., proportion, expectancy, std, etc.)
   
   2. State the null hypothesis `\(H_0\)`
   
   3. Specify the alternative hypothesis `\(H_1\)` (one sided, two sided, etc.) 
   
   4. Choose significance level `\(\alpha\)`
   
   5. Determine what test statistic to use (e.g., `\(Z, T, X^2\)`)
    
   6. State the rejection region for the statistic
   
   7. Compute the sample quantities, plug-in into the test statistic and compute it
   
   8. Decide if `\(H_0\)` should be rejected based on 6-7
   
--

Steps 1-4 must be done before starting the research (a-priori and not posteriori, we'll see why later on)

---

# Example for attraction effect hypothesis test decision rule (computation)

Let's create an `\(\alpha=0.05\)` decision rule for classifying if the attraction effect worked in our experiment.

--

.small[
  * The parameter is `\(p\)` (vaccinations after intervention)
]
.small[
  * `\(H_0\)`: p = 0.2 (no attraction effect)
]
.small[
  * `\(H_1\)`: p &gt; 0.2 (interevntion was successful)
]
.small[
  * `\(\alpha = 0.05\)`
]

--

.small[
   
   * The test statistic
   
`$$z_0 = \frac{x-np_0}{\sqrt{np_0(1-p_0)}}$$`

   * Reject `\(H_0\)` if `\(z_0&gt;z_{1-\alpha}=1.96\)`
   
   * Compute `\(z_0\)`, given experiment results: `\(x=29\)` successes, `\(n = 100\)` subjects
   
`$$z_0 = \frac{29-100\times0.2}{\sqrt{100\times0.2\times0.8}}=2.25&gt;1.96 = z_{0.95}$$`
]

--

.small[

   * Conclusion: we reject `\(H_0\)`, and therefor claim that the experiment nudged patients to vaccinate.
]

---
   
# Example for attraction effect hypothesis test decision rule (code)

.small[
In R, we have a number of functions for this specific test. The previous computation is an **apporximation** (doesn't work for small `\(n\)` or extreme `\(p\)`). R can compute an approximate or an exact test.
]

.tiny[

```r
prop.test(x = 29, n = 100, p = 0.2, alternative = "greater")
```

```
## 
## 	1-sample proportions test with continuity correction
## 
## data:  29 out of 100, null probability 0.2
## X-squared = 4.5156, df = 1, p-value = 0.01679
## alternative hypothesis: true p is greater than 0.2
## 95 percent confidence interval:
##  0.2171785 1.0000000
## sample estimates:
##    p 
## 0.29
```

```r
binom.test(x = 29, n = 100, p = 0.2, alternative = "greater")
```

```
## 
## 	Exact binomial test
## 
## data:  29 and 100
## number of successes = 29, number of trials = 100, p-value =
## 0.02002
## alternative hypothesis: true probability of success is greater than 0.2
## 95 percent confidence interval:
##  0.2158797 1.0000000
## sample estimates:
## probability of success 
##                   0.29
```
]

---

# P-value versus specified `\(\alpha\)` value

The results in R do not state a specific `\(z_0\)` but rather a **p-value**. What does p-value mean?

   * By stating "we reject `\(H_0\)` at the `\(\alpha\)` level" we're omitting important information:

      * How "significant" is our rejection? 
      * Would this rejection hold when we vary `\(\alpha\)`? i.e.: Would our rejection hold at `\(\alpha=0.03\)`? `\(\alpha=0.0001\)`?

--

* The p-value is the smallest level of significance that would lead to rejection of the null hypothesis `\(H_0\)` with the given data.

--

`$$z_0 = \frac{29-100\times0.2}{\sqrt{100\times0.2\times0.8}}=2.25&gt;1.65 = z_{0.95} = z_{1-\alpha}$$`

As long as `\(z_{1-\alpha}\)` &lt; 2.25 we would still reject `\(H_0\)`. Hence, the p-value is:

`$$\text{p-value}=1-\Phi(z_0=2.25) = 0.012$$`

.small[

```r
1-pnorm(q=2.25)
```

```
## [1] 0.01222447
```
]

---

# Type-II error and determining the sample size (1/3)

So far, we've only considered the choice of `\(\alpha\)`, i.e., the type-I error. However, we can also influence `\(\beta\)` by choice of the sample size `\(n\)` (and sometimes also by the type of test - *out of scope*).

Let's see how change in `\(n\)` can increase the power (decrease the type-II error, `\(\beta\)`).

--

In our example, we had `\(p_0=0.2\)`, let's assume now that in the alternative hypothesis we use `\(p_1=p_0+\delta\)` (one sided "greater than" test).

--

`$$\beta=P(\text{Not rejecting } H_0 | H_1 \text{ true}) = P\left(\frac{x-np_0}{\sqrt{np_0(1-p_0)}}\leq z_{1-\alpha} | p_1=p_0+\delta\right)$$`

--

Note that

`$$Z_0=\frac{x-np_0}{\sqrt{np_0(1-p_0)}}=\frac{x-(np_0 + n\delta)}{\sqrt{np_0(1-p_0)}} + \frac{\delta\sqrt{n}}{\sqrt{p_0(1-p_0)}}$$`

Given `\(H_0\)`, the distribution of `\(Z_0\sim\mathcal{N}\left(\delta\sqrt{n/p_0(1-p_0)}, 1\right)\)`, hence

---

# Type-II error and determining the sample size (2/3)

`$$\beta = P\left(\frac{x-(np_0+n\delta)}{\sqrt{np_0(1-p_0)}}&lt;z_{1-\alpha} - \frac{\delta\sqrt{n}}{\sqrt{p_0(1-p_0)}} | p_1=p_0+\delta\right) =$$` 
`$$=P\left(\frac{x-(np_0+n\delta)}{\sqrt{n(p_0+\delta)(1-p_0-\delta)}}&lt;\frac{z_{1-\alpha}\sqrt{np_0(1-p_0)}-\delta n}{\sqrt{n(p_1+\delta)(1-p_1-\delta)}}\quad|\quad p_0+\delta\right)$$`

`$$\beta=\Phi\left(\frac{z_{1-\alpha}\sqrt{np_0(1-p_0)}-\delta n}{\sqrt{n(p_0+\delta)(1-p_0-\delta)}}\right)$$`

Now we can use `qnorm(beta)`, i.e.: `\(\Phi^{-1}(\beta)\)`, and given `\(\beta\)`, `\(p_0\)`, `\(\delta\)` compute `\(n\)`.

--

Luckily, we have a function for that. From package `pwr`, function `pwr.p.test`.

---

# Type-II error and determining the sample size (3/3)

.tiny[

```r
beta &lt;- 0.2
delta &lt;- 0.05
alpha &lt;- 0.05
p_0 &lt;- 0.2

library(pwr)
```

```
## Warning: package 'pwr' was built under R version 3.6.1
```

```r
p_out &lt;- pwr::pwr.p.test(h = ES.h(p1 = p_0 + delta, p2 = p_0),
                sig.level = alpha,
                power = 1-beta,
                alternative = "greater")
p_out
```

```
## 
##      proportion power calculation for binomial distribution (arcsine transformation) 
## 
##               h = 0.1199023
##               n = 430.044
##       sig.level = 0.05
##           power = 0.8
##     alternative = greater
```
]
.right-plot[
&lt;img src="03-Hypothesis_tests_files/figure-html/plot-1.png" style="display: block; margin: auto;" /&gt;
]

---

# One-tailed versus two-tailed hypothesis

So far, in our example, we've used a one-tailed hypothesis:

   * `\(H_0\)`: `\(p=0.2\)`
   * `\(H_1\)`: `\(p&gt;0.2\)`

--
   
Depending on context, we can also have the other direction:
   
   * `\(H_0\)`: `\(p=0.2\)`
   * `\(H_1\)`: `\(p&lt;0.2\)`
   
--

Or a two sided hypothesis:

   * `\(H_0\)`: `\(p=0.2\)`
   * `\(H_1\)`: `\(p\neq0.2\)`
   
---

# Two tailed hypothesis in the vaccination example

If we were to use a two tailed hypothesis (the attraction had some effect, either increase or decrease vaccination rates), then our rejection criteria would become:

   * Reject `\(H_0\)` if `\(z_0&gt;z_{1-\alpha/2}\)` or `\(z_0&lt;z_{\alpha/2}\)`

--
   
   * I.e., for `\(\alpha=0.05\)`:
   * Reject `\(H_0\)` if `\(z_0&gt;1.96\)` or `\(z_0&lt;-1.96\)`

--

The p-value would be in this case:

`$$\text{p-value}=2[1-\Phi(|z_0|)] = 0.0244$$`


```r
2*(1-pnorm(q = 2.25))
```

```
## [1] 0.02444895
```

---

# The relationship between hypothesis testing and confidence intervals

For confidence intervals of the proportion `\(p\)` we used:

`$$1-\alpha = P\left(\hat{p}+z_{\alpha/2}\sqrt{p(1-p)/n}&lt;p&lt;\hat{p}+z_{1-\alpha/2}\sqrt{p(1-p)/n}\right)$$`

.small[(We had similar confidence intervals for the average of a population normally distributed, for variance, etc.)]

--

Negating the expression `\(\ldots\)` within the probability `\(P(\ldots)\)`, we get:

`$$\alpha = 1-(1-\alpha) = 1-P\left(\hat{p}+z_{\alpha/2}\sqrt{p(1-p)/n}&lt;p&lt;\hat{p}+z_{1-\alpha/2}\sqrt{p(1-p)/n}\right)=$$`

`$$=P\left(\hat{p}+z_{\alpha/2}\sqrt{p(1-p)/n}\geq p\right)+P\left(p\leq\hat{p}+z_{1-\alpha/2}\sqrt{p(1-p)/n}\right)$$`

--

When the parameter under `\(H_0\)` is "outside" a 95% confidence interval based on the sample, the null hypothesis is rejected at `\(\alpha=0.05\)`

   * E.g., `\(p=0.2\)` in the previous example is outside the confidence interval for `\(p\in(0.216,1)\)`.

---

# Hypothesis testing for expectancy - normal distribution

In this example, we implement a two sided hypothesis test on the power lifting data.
.tiny[

```r
# data source:
#https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-08/ipf_lifts.csv
male_deadlift &lt;- read_csv("data/ipf_lifts.csv", col_types = cols()) %&gt;% 
   filter(sex == "M") %&gt;% 
   select(best3deadlift_kg) %&gt;% 
   filter(best3deadlift_kg &gt; 0) %&gt;% 
   sample_n(500)

ggplot(male_deadlift, aes(best3deadlift_kg)) + 
   geom_histogram() + 
   theme_bw()
```

```
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
```

&lt;img src="03-Hypothesis_tests_files/figure-html/power lifting-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Hypothesis testing for expectancy - normal distribution - power lifting example

Parameter `\(\mu\)` is the expected weight a power lifter can pull in a deadlift
   
   * `\(H_0\)`: `\(\mu=225\)` kg
   * `\(H_0\)`: `\(\mu\neq225\)` kg
   * `\(\alpha=0.05\)`
   * Test statistic `\(T_0 = \frac{\bar{X}-\mu_0}{s/\sqrt{n}}\)`
   * `\(T_0 &lt; t_{\alpha/2,n-1}\)` or `\(T_0&gt; t_{1-\alpha/2,n-1}\)`
.tiny[

```r
t.test(x = male_deadlift$best3deadlift_kg, 
       alternative = "two.sided", 
       mu = 225, conf.level = 0.95)
```

```
## 
## 	One Sample t-test
## 
## data:  male_deadlift$best3deadlift_kg
## t = 12.632, df = 499, p-value &lt; 2.2e-16
## alternative hypothesis: true mean is not equal to 225
## 95 percent confidence interval:
##  248.0030 256.4768
## sample estimates:
## mean of x 
##  252.2399
```
]

---

# Comparing distributions - qqplot

So far, we discussed tests related to parameters' values, however, sometimes we want to compare entire distributions. For example, is the deadlift max weight normally distributed?

--

A qqplot draws the sample (on y axis) versus the theoretical distribuion (on x-axis). If the two are on `\(y=x\)` this means that the distributions match. 

--

Would you say that the following are from the same distribution?
.tiny[

```r
ggplot(male_deadlift, aes(sample = best3deadlift_kg)) + 
   geom_qq(distribution = stats::qnorm, dparams = list(mean = 250, sd = 50)) + 
   theme_bw() + 
   geom_abline(slope = 1, intercept = 0)
```

&lt;img src="03-Hypothesis_tests_files/figure-html/deadlift weight qqplot-1.png" style="display: block; margin: auto;" /&gt;
]

---

# Comparing distributions - qqplot - another example

Now with both genders.
.tiny[

```r
set.seed(0)
deadlift &lt;- read_csv("data/ipf_lifts.csv", col_types = cols()) %&gt;% 
   select(best3deadlift_kg) %&gt;% 
   filter(best3deadlift_kg &gt; 0) %&gt;% 
   sample_n(500)

ggplot(deadlift, aes(sample = best3deadlift_kg)) + 
   geom_qq(distribution = stats::qnorm, dparams = list(mean = 223, sd = 63)) + 
   theme_bw() + 
   geom_abline(slope = 1, intercept = 0)
```

&lt;img src="03-Hypothesis_tests_files/figure-html/qqplot example-1.png" style="display: block; margin: auto;" /&gt;
]

--

Looks like there's a heavy tail towards higher weights in the sample than the theoretical. How do we quantify this as a statistical hypothesis test of "goodness of fit"?

---

# Hypothesis testing - goodness of fit

Goodness of fit tests are used to test how good is the fit of our empirical distribution to that of a theoretical distribution.

--

Arrange the empirical distribution in `\(k\)` bins, and let `\(O_i\)` be the observed frequency in the `\(i\)`th class bin. Let `\(E_i\)` be the expected probability. The test statistic is:

`$$\chi^2_0=\sum_{i=1}^k\frac{(O_i-E_i)^2}{E_i}$$`

If the population follows the hypothesized distribution, then the expression is approximately distributed `\(\chi^2_{k-p-1}\)`, where `\(p\)` os the number of parameters of the hypothesized distribution estimated by sample statistics.

The approximation improves as `\(n\)` increases.

--

When using the approximation, make sure that `\(E_i\)` is "big enough" (i.e., `\(E_i\geq5, \forall i\)`)

---

# Hypothesis testing - goodness of fit - procedure

We would reject the hypothesis if `\(\chi_0^2&gt;\chi_{\alpha,k-p-1}^2\)`.

   1. We are interested in the form of the distribution of maximum deadlift weight
   2. `\(H_0\)`: The deadlift weight is normally distributed
   3. `\(H_1\)`: The deadlift weight is not normally distributed
   4. `\(\alpha = 0.05\)`
   5. The test statistic is: `\(\chi^2_0=\sum_{i=1}^k{\frac{(O_i-E_i)^2}{E_i}}\)`
   6. Reject `\(H_0\)` if `\(\chi_0^2&gt;\chi^2_{0.05,k-p-1}\)`

---

# Goodness of fit - example

.tiny[

```r
interval_breaks &lt;- c(0, 120, 190, 250, 320, 600)
sample_size &lt;- 500

deadlift_gfit &lt;- deadlift %&gt;% 
   mutate(weight_gr = cut(best3deadlift_kg, breaks = interval_breaks))

mu &lt;- mean(deadlift_gfit$best3deadlift_kg)
sigma &lt;- sd(deadlift_gfit$best3deadlift_kg)

deadlift_chi_prep &lt;- deadlift_gfit %&gt;% 
   count(weight_gr, name = "observed") %&gt;% 
   mutate(upper_bound = interval_breaks[-1]) %&gt;% 
   mutate(lower_bound = interval_breaks[1:5]) %&gt;% 
   mutate(expected_prob = pnorm(q = upper_bound, mean = mu, sd = sigma)-
             pnorm(q = lower_bound, mean = mu, sd = sigma)) %&gt;% 
   mutate(expected_prob =  expected_prob/sum(expected_prob)) %&gt;% 
   mutate(expected = expected_prob*500) %&gt;% 
   mutate(chi_comp = (observed-expected)^2/expected)

chi2_0 &lt;- sum(deadlift_chi_prep$chi_comp)
chi2_0
```

```
## [1] 26.35624
```

```r
qchisq(p = 1-0.05, df = 5-1)
```

```
## [1] 9.487729
```
]

Since `\(26.3=\chi^2_0&gt;\chi^2_{500, 3}=9.49\)` we reject the null hypothesis. This distribution is not normal.

What is the p-value? (really small)

--

.tiny[

```r
1-pchisq(q = chi2_0, df = 5-1)
```

```
## [1] 2.681849e-05
```
]

---

# Goodness of fit - example - using R's *chisq.test* command


```r
chisq.test(x = deadlift_chi_prep$observed, 
           p = deadlift_chi_prep$expected_prob)
```

```
## 
## 	Chi-squared test for given probabilities
## 
## data:  deadlift_chi_prep$observed
## X-squared = 26.356, df = 4, p-value = 2.682e-05
```

---

# Words of caution 

Two common mistakes when analyzing data are:

   1. HARKing (Hypothesizing After Results are Known)
   2. Using multiple comparisons without compensating for them

In essense, Since we have a `\(\alpha\)` error rate (e.g. 5%), when we perform 100 hypothesis tests we are bound to get 5% false positives.

--

For (1) that's something you shouldn't do! Some organizations offer *preregistration* as means of avoiding HARKing and improving research. I.e., [https://cos.io/prereg/](https://cos.io/prereg/)

--

For (2) there are procedures to control the false discovery rate (FDR). In R, there is a command called `p.adjust` for example. We might talk about this, if time permits.

---

# Abusing hypothesis tests - example (don't try this at home!)

.tiny[

```r
set.seed(0)

# let's get random numbers
random_normal &lt;- matrix(rnorm(n=100*100, mean = 0, sd = 1),
                        nrow = 100, ncol = 100) %&gt;% 
   as_tibble(.name_repair = "unique")
```

```
## New names:
## * `` -&gt; ...1
## * `` -&gt; ...2
## * `` -&gt; ...3
## * `` -&gt; ...4
## * `` -&gt; ...5
## * ... and 95 more problems
```
]

---

# Abusing hypothesis tests - example (don't try this at home!)

.tiny[

```r
# we have 100 variables which are standard normal distributed
# let's take the one with the highest average
random_normal %&gt;% 
   pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %&gt;%
   group_by(variable) %&gt;% 
   summarize(mean_val = mean(value)) %&gt;% 
   arrange(desc(mean_val))
```

```
## # A tibble: 100 x 2
##    variable mean_val
##    &lt;chr&gt;       &lt;dbl&gt;
##  1 ...78       0.287
##  2 ...28       0.244
##  3 ...35       0.233
##  4 ...53       0.228
##  5 ...60       0.224
##  6 ...100      0.211
##  7 ...97       0.191
##  8 ...77       0.173
##  9 ...99       0.166
## 10 ...67       0.165
## # ... with 90 more rows
```

```r
# H_0: this variable (..78) has mu=0, H_1: otherwise

t.test(random_normal$...78)
```

```
## 
## 	One Sample t-test
## 
## data:  random_normal$...78
## t = 2.5858, df = 99, p-value = 0.01117
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.06668831 0.50660635
## sample estimates:
## mean of x 
## 0.2866473
```
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
